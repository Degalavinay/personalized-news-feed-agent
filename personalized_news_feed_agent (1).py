# -*- coding: utf-8 -*-
"""Personalized_News_Feed_Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_iQdbyapgfgGJ5ucdUF4VnlGN2yaedto
"""

# Import libraries
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import os

# Set up environment for HuggingFace model
import os
os.environ["HUGGINGFACE_HUB_NO_TOKEN"] = "1"

# Download NLTK resources
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)  # Fixed error for missing 'punkt_tab'
stop_words = set(stopwords.words('english'))

# Create directories in Google Drive (or local path)
output_dir = 'Downloads'
os.makedirs(f'{output_dir}/data', exist_ok=True)
os.makedirs(f'{output_dir}/plots', exist_ok=True)

# Load dataset (adjust the path if needed)
data_path = r'C:/Users/degal/Downloads/archive/News_Category_Dataset_v3.json'
data = pd.read_json(data_path, lines=True)

# Handle missing values and sample 1000 articles
data = data.dropna(subset=['headline', 'short_description'])
data = data.sample(1000, random_state=42).reset_index(drop=True)

# Combine headline and short_description
data['text'] = data['headline'] + ' ' + data['short_description']

# Text preprocessing
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

data['clean_text'] = data['text'].apply(preprocess_text)

# Encode categories
le = LabelEncoder()
data['category_id'] = le.fit_transform(data['category'])

# Save preprocessed data
data.to_csv(f'{output_dir}/data/preprocessed_news.csv', index=True)

# Simulate user interactions
num_users = 50
users = [f'user_{i}' for i in range(num_users)]
np.random.seed(42)
user_preferences = {
    user: np.random.choice(data['category'].unique(), size=np.random.randint(2, 5), replace=False)
    for user in users
}

interaction_matrix = pd.DataFrame(0, index=users, columns=data.index)
for user in users:
    preferred_categories = user_preferences[user]
    preferred_articles = data[data['category'].isin(preferred_categories)].index
    num_available_articles = len(preferred_articles)
    num_interactions = min(np.random.randint(5, 15), num_available_articles)
    if num_interactions > 0:
        interacted_articles = np.random.choice(preferred_articles, size=num_interactions, replace=False)
        interaction_matrix.loc[user, interacted_articles] = 1

interaction_matrix.to_csv(f'{output_dir}/data/interaction_matrix.csv')

# Generate article embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
article_embeddings = model.encode(data['clean_text'].tolist(), show_progress_bar=True)
article_embeddings_df = pd.DataFrame(article_embeddings, index=data.index)
article_embeddings_df.to_csv(f'{output_dir}/data/article_embeddings.csv')

# Generate user profiles
user_profiles = pd.DataFrame(0, index=users, columns=range(article_embeddings.shape[1]))

# Correcting the casting
user_profiles = user_profiles.astype('float64')

# Update user profiles based on interactions
for user in users:
    interacted_articles = interaction_matrix.loc[user][interaction_matrix.loc[user] == 1].index
    if len(interacted_articles) > 0:
        user_profile = article_embeddings[interacted_articles].mean(axis=0)
        user_profiles.loc[user] = user_profile

user_profiles.to_csv(f'{output_dir}/data/user_profiles.csv')

# Collaborative filtering
user_similarity = cosine_similarity(user_profiles)
user_similarity_df = pd.DataFrame(user_similarity, index=users, columns=users)

def get_collaborative_recommendations(user, num_recommendations=10):
    similar_users = user_similarity_df[user].sort_values(ascending=False)[1:6]
    similar_users_articles = interaction_matrix.loc[similar_users.index]
    article_scores = similar_users_articles.sum(axis=0)
    already_interacted = interaction_matrix.loc[user][interaction_matrix.loc[user] == 1].index
    article_scores[already_interacted] = 0
    recommended_articles = article_scores.sort_values(ascending=False).head(num_recommendations).index
    return recommended_articles

# Content-based filtering
def get_content_based_recommendations(user, num_recommendations=10):
    user_embedding = user_profiles.loc[user].values.reshape(1, -1)
    similarities = cosine_similarity(user_embedding, article_embeddings_df)
    similarities = pd.Series(similarities[0], index=data.index)
    already_interacted = interaction_matrix.loc[user][interaction_matrix.loc[user] == 1].index
    similarities[already_interacted] = -1
    recommended_articles = similarities.sort_values(ascending=False).head(num_recommendations).index
    return recommended_articles

# Hybrid recommendations
def get_hybrid_recommendations(user, num_recommendations=10, cf_weight=0.5):
    cf_recs = get_collaborative_recommendations(user, num_recommendations * 2)
    cb_recs = get_content_based_recommendations(user, num_recommendations * 2)
    cf_scores = {idx: (1 - i / len(cf_recs)) * cf_weight for i, idx in enumerate(cf_recs)}
    cb_scores = {idx: (1 - i / len(cb_recs)) * (1 - cf_weight) for i, idx in enumerate(cb_recs)}
    combined_scores = {}
    for idx in set(cf_recs).union(cb_recs):
        combined_scores[idx] = cf_scores.get(idx, 0) + cb_scores.get(idx, 0)
    recommended_articles = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:num_recommendations]
    return [idx for idx, _ in recommended_articles]

# Display recommendations
def display_recommendations(user, recommendations):
    print(f"Personalized News Feed for {user}:")
    for i, idx in enumerate(recommendations[:5], 1):
        article = data.loc[idx]
        print(f"{i}. {article['headline']}")
        print(f"   Summary: {article['short_description']}")
        print(f"   Category: {article['category']}\n")

# Test recommendations
user = 'user_0'
hybrid_recs = get_hybrid_recommendations(user)
display_recommendations(user, hybrid_recs)

# Simulate feedback
new_interaction = hybrid_recs[0]
interaction_matrix.loc[user, new_interaction] = 1
interacted_articles = interaction_matrix.loc[user][interaction_matrix.loc[user] == 1].index
user_profiles.loc[user] = article_embeddings_df.loc[interacted_articles].mean(axis=0)
new_recs = get_hybrid_recommendations(user)
print("\nAfter Feedback:")
display_recommendations(user, new_recs)

# Visualizations
user_interactions = data.loc[interaction_matrix.loc[user][interaction_matrix.loc[user] == 1].index]
category_counts = user_interactions['category'].value_counts()
plt.figure(figsize=(8, 5))
category_counts.plot(kind='bar')
plt.title(f'Category Preferences for {user}')
plt.xlabel('Category')
plt.ylabel('Number of Interactions')
plt.tight_layout()
plt.savefig(f'{output_dir}/plots/user_preferences.png')
plt.show()

recommended_categories = data.loc[hybrid_recs]['category'].value_counts()
plt.figure(figsize=(8, 5))
recommended_categories.plot(kind='bar')
plt.title(f'Recommended Article Categories for {user}')
plt.xlabel('Category')
plt.ylabel('Number of Articles')
plt.tight_layout()
plt.savefig(f'{output_dir}/plots/recommended_categories.png')
plt.show()

# Verify user preferences
print(f"User Preferences for {user}: {user_preferences[user]}")

